#read/《A_Common-Sense_Guide_to_Data_Structures_and_Algorithms》 #CS/07dataStructure 

- [book](https://usermanual.wiki/Document/A20Common20Sense20Guide20To20Data20Structures20And20Algorithms2020Jay20Wengrow.1281995573.pdf)
- [exercise](https://commonsensecomputerscience.com/old-site/index.html)

- Content-p6
- concepts
	- when we measure **how “fast” an operation takes**, we do not refer to ~~how fast the operation takes in terms of pure time~~, but instead in **how many steps it takes**.
- Chapter 1-p21 Why Data Structures Matter 
	- p23 The **Array**: The Foundational Data Structure
		- **operations**
			- **Read**: Reading refers to looking something up from a particular spot within the data structure. **One step**.
				- eg: `array[0]`, which grocery item is located at index 2 would be reading from the array.
			- **Search**: Searching refers to looking for a particular value within a data structure. 
				- ==**linear search algorithm**==: for N cells in an array, linear search will take a **maximum of N steps**. 
					- eg: `array.include("value")`, if "dates" is in our grocery list, and which index it’s located at would be searching the array.
				- ==**binary search algorithm**== (in chapter 2, can only be used in ordered arrays)
			- **Insert**: Insertion refers to adding another value to our data structure. 
				- **at the end** of an array: **one step**. eg: `array.push("value")`, add "value" to our shopping list.
				- **at the beginning or the middle** of an array: for N cells in an array, insert will take a **maximum of N+1 steps**.  
			- **Delete**: Deletion refers to removing a value from our data structure. eg: `???`, removed "bananas" from our grocery list
				- **at the end** of an array: **one step**. 
				- **at the beginning or the middle** of an array: for N cells in an array, delete will take a **maximum of N steps**.  
	- p38 **Sets**: How a Single Rule Can Affect Efficiency
		- **array-based set**.
			- **Insert** (where arrays and sets diverge): **search (if duplicate) + insertion**
				- **at the end** of an array: for N cells in an array, insert will take a **maximum of N+ 1 steps**. 
				- **at the beginning or the middle** of an array: for N cells in an array, insert will take a **maximum of 2 N+1 steps**. 
	- ==Q: avoid sets just because insertion is slower for sets than regular arrays? ==
		- A: Absolutely not. 
			- **Sets** are important ***when you need to ensure that there is no duplicate data***. 
			- But ***when you don’t have such a need***, an **array may be preferable**, since **insertions for arrays are more efficient** than insertions for sets.
- Chapter 2-p43 Why Algorithms Matter 
	- **algorithm**: a particular **process for solving a problem**. When applied to computing, an algorithm refers to a **process for going about a particular operation**.
		- eg: the process for preparing a bowl of cereal can be called an algorithm. The cerealpreparation algorithm follows these four steps (for me, at least): 1. Grab a bowl. 2. Pour cereal in the bowl. 3. Pour milk in the bowl. 4. Dip a spoon in the bowl.
	- **Ordered Arrays**: The major advantage of an ordered array over a standard array is that we **have the option of performing a binary search** rather than a linear search.
		- **Search**
			- ==**linear search algorithm**==: (in chapter 1, for N cells in an array, linear search will take a **maximum of N steps**)				
			- ==**binary search algorithm**==: Step1: We begin our search from the **central cell**. Step2: Among the cells **to the left of the central cell**, we inspect the **middlemost value**...repeat...StepN: We inspect the final remaining cell. (If it’s not there, that means that there is no "value" within this ordered array.)
				- The pattern that emerges is that for **every time we double the number of items in the ordered array**, the **number of steps needed for binary search increases by just one**.
		- Insert: **search (for the appropriate place) + insertion**
			- **at the beginning or the middle** of an array: for N cells in an array, Insert will take a **maximum of 2 N +1 steps**. 
		- **Delete**: **search (for the appropriate place) + delete**
			- **at the beginning or the middle** of an array: for N cells in an array, Insert will take a **maximum of 2 N +1 steps**.
	- ==Q: always use ordered arrays just because ordered arrays allow for binary search?==
		- A: No.
			- In situations ~~where you don’t anticipate searching the data much~~, **but only adding data**, **standard arrays may be a better choice** because their **insertion is faster**.
- Chapter 3-p59 Oh Yes! Big O Notation
	- Describe the efficiency of this algorithm in **Big O Notation**(borrowed from mathematic word).
		- eg: the upper bound of the growth rate of a function, or that **if** a function g(x) **grows no faster than** a function f(x), then g is said to be a member of O(f).
	- Big O: describes the **number of steps that an algorithm takes**; or describes how many **steps an algorithm takes based on the number of data elements** that **the algorithm is acting upon**.
		- eg: array reading, or insertion and deletion of a value at the end of an array takes only one step, then count **"O(1)"** (pronounce “Oh of 1.”/“Big Oh of 1.”/“Order of 1.”), simply means that **the algorithm takes the same number of steps** no matter how much data there is.
		- eg: for N elements in the array, linear search can take up to a maximum of N steps, then use "**O(N)**" (pronounce “Oh of N.”)
	- Constant Time vs. Linear Time ![[Pasted image 20230607234104.png]]
		- **O(1)**, a **perfect horizontal line**, referred to as **constant time**. So, an **algorithm can be described as O(1) even if it takes more than one step**.![[Pasted image 20230607234719.png]] 
		- ==Because there will always be some amount of data in which the tides turn, and O(N) takes more steps from that point until infinity, **O(N)** is considered to be, **on the whole**, **less efficient** than O(1).==[^1]
	- Same Algorithm, Different Scenarios
		- eg: linear search is O(1) in a best-case scenario, and O(N) in a worst-case scenario.
	- An Algorithm of the Third Kind
		- eg: use "**O(log N)**"(pronounce “Oh of log N.”) , shorthand for saying O(log2 N), to count binary search algorithm. It **takes just one additional step** every time the **data elements are doubled**.![[Pasted image 20230608073511.png]]
- Chapter 4-p76 Speeding Up Your Code with Big O
	- **Sorting algorithms** solve problem: Given an array of unsorted numbers, how can we sort them so that they end up in ascending order?
		- eg: ==**Bubble Sort algorithm**== (in each passthrough, the highest unsorted value “bubbles” up to its correct position) is a very basic sorting algorithm, and follows these steps: 1. **Point to two consecutive items** in the array. (Initially, we start at the very beginning of the array and point to its first two items.) **Compare** the first item with the second one; 2. ***If the two items are out of order*** (in other words, the left value is greater than the right value), **swap them**; 3. **Move the “pointers” one cell to the right** and **repeat steps 1 and 2** ***until we reach the end of the array*** or any items that have already been sorted.; 4. **Repeat steps 1 through 3** ***until*** we have a round in which we ***didn’t have to make any swaps***. This means that the array is in order.
			- for N elements, we make **(N - 1) + (N - 2) + (N - 3) … + 1** comparisons. In a **worst-case scenario**, where the array is not just randomly shuffled, but sorted in **descending order** (the exact opposite of what we want), we’d actually need a **swap for each comparison**. It’s growing by **approximately N²**. Therefore, in Big O Notation, we would say that Bubble Sort has an efficiency of **O(N²)**. ![[Pasted image 20230608233851.png]]
		- ==**Selection Sort**== (see Chapter 5) 
		- ==**Insertion Sort algorithm**== (see Chapter 6)  
		- **O(N²)** is considered to be a **relatively inefficient algorithm**, since as the data increases, the steps increase dramatically. ![[Pasted image 20230608234344.png]]
			- ==***Whenever encountering a slow algorithm***, it’s **worth spending some time to think** about **whether there may be any faster alternatives**.==
		- O(N²) is also referred to as **quadratic time**. When you see a **nested loop**, O(N²) alarm bells should start going off in your head. 
			- eg: Quadratic Problem![[Pasted image 20230609072703.png]]
				- ==???Linear Solution![[Pasted image 20230609072347.png]]
- Chapter 5-p96 Optimizing Code with and Without Big O
	- **Big O** is certainly ~~not the only tool~~. 
		- ==**Bubble Sort algorithm**== (see Chapter 4)  
		- eg: ==**Selection Sort algorithm**== with these steps: 1. We **check each cell of the array** from left to right to **determine which value is least**. As we move from cell to cell, we keep in a **variable** the lowest value we’ve encountered so far. 2. ***Once we’ve determined which index contains the lowest value***, we **swap** that index **with the value we began the passthrough with**. 3. **Repeat steps 1 and 2** ***until all the data is sorted***. 
			- for N elements, we make **(N - 1) + (N - 2) + (N - 3) … + 1 comparisons**. As for swaps, however, we only need to make a **maximum of one swap per passthrough**. ![[Pasted image 20230610120426.png]]
		- ==**Insertion Sort algorithm**== (see Chapter 6)  
	- **Ignoring Constants**
		- ==In reality, however, Selection Sort is described in Big O as O(N² ), just like Bubble Sort. This is because of a major rule of Big O that we’re now introducing for the first time: **Big O Notation ignores constants.**==[^2] 
			- The purpose of Big O is that for different classifications, there will be a point at which one classification supersedes the other in speed, and will **remain faster forever**. ***When that point occurs exactly***, however, ~~is not the concern of Big O~~.
				- eg: In our case, what should technically be O(N² / 2) becomes simply O(N² ). Similarly, O(2N) would become O(N), and O(N² / 2) would also become O(N). Even O(100N), which is 100 times slower than O(N), would also be referred to as O(N). ![[Pasted image 20230610121829.png]]
	- ==Q: how to discern between two algorithms that seem to have the same efficiency, and select the faster of the two?
		- A: further analysis is required.
			- eg: comparing "N² steps' O(N²)" and "N² / 2 steps' O(N²)". **Although** the two algorithms fall under the same classification--O(N), but the second one is more efficient.
- Chapter 6-p116 Optimizing for Optimistic Scenarios
	- **The worst-case scenario** ~~isn’t the only situation worth considering~~.
		- ==**Bubble Sort algorithm**== (see Chapter 4)  
		- ==**Selection Sort algorithm**== (see Chapter 5)  
		- eg: ==**Insertion Sort algorithm**== with these steps: 1. In the first passthrough, we **temporarily remove the value at index 1 (the second cell)** and **store it in a temporary variable**. This will **leave a gap at that index**. 2. ***If the value to the left of the gap*** is ***greater than the temporary variable***, we **shift that value to the right**. As we shift values to the right, inherently, the **gap moves leftwards**. 3. We then **insert the temporarily removed value** into the **current gap**. 4. We **repeat steps 1 through 3** ***until the array is fully sorted***.
			- for N elements, 
				- in a worst-case scenario, where the array is sorted in reverse order, we have to compare every number to the left of temp_value with temp_value in each passthrough. we make **1 + 2 + 3 + … + (N - 1)  comparisons**, and **as many shifts as there are comparisons**, and **N - 1 removals** and **N - 1 insertions**. In total O(N² + 2N - 2) which equals O(N² )
				- in a best-case scenario, where the array is sorted in order, we make **N - 1  comparisons, no shift, no removal, no insertion**. In total O(N - 1) which equals O(N )
		- ==Only consider the greatest order of N==[^3]
			- eg: O(N² + N) simply becomes O(N² ). Because as N increases, N² becomes so much more significant than any other order of N. ![[Pasted image 20230610144432.png]]
	- ==Q: Which is better: Selection Sort or Insertion Sort? ==
		- A: It depends. 
			- In an ***average case—where an array is randomly sorted***—they perform similarly.
			- If you ***have reason to assume*** that you’ll be dealing with data that is ***mostly sorted***, **Insertion Sort will be a better choice**. 
			- If you have reason to assume that you’ll be dealing with data that is ***mostly sorted in reverse order***, **Selection Sort will be faster**. 
			- If you ***have no idea what the data will be like***, that’s essentially an average case, and **both will be equal**.
	- Having the **ability to ==discern between best-, average-, and worst-case scenarios**== is a key skill in choosing the best algorithm for your needs, as well as ==**taking existing algorithms and optimizing them**== further to make them significantly faster. Remember, while it’s good to be prepared for the worst case, average cases are what happen most of the time. 
	- Just as you now have the ability to choose between two competing algorithms for a given use case, you’ll also need the **ability to ==choose between two competing data structures**==, as one may have better performance than the other.
- Chapter 7-p138 Blazing Fast Lookup with Hash Tables
- Chapter 8-p158 Crafting Elegant Code with Stacks and Queues
- 
- Chapter 9-p173 Recursively Recurse with Recursion
- Chapter 10-p187 Recursive Algorithms for Speed
- 
- Chapter 11-p212 Node-Based Data Structures
- Chapter 12-p234 Speeding Up All the Things with Binary Trees
- Chapter 13-p257 Connecting Everything with Graphs
- 
- Chapter 14-p288 Dealing with Space Constraints